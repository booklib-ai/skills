{
  "evals": [
    {
      "id": "eval-01-no-rate-limiting-no-error-handling-no-robots",
      "prompt": "Review this web scraper:\n\n```python\nimport requests\nfrom bs4 import BeautifulSoup\nimport json\n\nBASE_URL = 'https://books.example.com'\n\ndef scrape_all_books():\n    all_books = []\n    page = 1\n\n    while True:\n        url = f'{BASE_URL}/catalogue/page-{page}.html'\n        response = requests.get(url)\n        soup = BeautifulSoup(response.text, 'html.parser')\n\n        books = soup.find_all('article', class_='product_pod')\n        if not books:\n            break\n\n        for book in books:\n            title = book.find('h3').find('a')['title']\n            price = book.find('p', class_='price_color').text\n            rating = book.find('p', class_='star-rating')['class'][1]\n            all_books.append({'title': title, 'price': price, 'rating': rating})\n\n        page += 1\n\n    return all_books\n\nresult = scrape_all_books()\nwith open('books.json', 'w') as f:\n    json.dump(result, f)\n```",
      "expectations": [
        "Flags no robots.txt check: the scraper does not check or respect the site's robots.txt before crawling (Ch 18: always check and honor robots.txt)",
        "Flags no rate limiting: requests are issued as fast as possible with no delay between pages; recommends adding `time.sleep()` of at least 1-3 seconds between requests (Ch 14: rate limit requests)",
        "Flags no error handling on `requests.get()`: a network error, timeout, or non-200 response will raise an exception or silently produce garbage HTML (Ch 1, 14: wrap requests in try/except, check response status)",
        "Flags no User-Agent header: the scraper uses the default requests User-Agent which may be blocked and does not identify the bot (Ch 14: set a descriptive User-Agent header)",
        "Flags no session reuse: `requests.get()` called in a loop creates a new connection for each page; recommends `requests.Session()` for connection pooling (Ch 10: use sessions for connection pooling)",
        "Flags defensive parsing issues: `book.find('h3').find('a')['title']` will raise AttributeError if any element is missing; recommends checking for None before accessing attributes (Ch 2: parse defensively)",
        "Flags no logging of progress or errors (Ch 5: log page fetches, errors, items extracted)"
      ]
    },
    {
      "id": "eval-02-regex-for-html-parsing",
      "prompt": "Review this data extraction code:\n\n```python\nimport requests\nimport re\n\ndef extract_product_data(url: str) -> dict:\n    response = requests.get(url)\n    html = response.text\n\n    # Extract product name\n    name_match = re.search(r'<h1[^>]*>([^<]+)</h1>', html)\n    name = name_match.group(1) if name_match else None\n\n    # Extract price\n    price_match = re.search(r'<span class=\"price\">\\$([\\d\\.]+)</span>', html)\n    price = float(price_match.group(1)) if price_match else None\n\n    # Extract description paragraphs\n    desc_matches = re.findall(r'<p class=\"desc\">(.+?)</p>', html, re.DOTALL)\n    description = ' '.join(desc_matches)\n\n    # Extract all href links on the page\n    links = re.findall(r'href=[\"\\']([^\"\\']+)[\"\\']', html)\n\n    # Check if in stock\n    in_stock = bool(re.search(r'<span class=\"stock\">In Stock</span>', html))\n\n    return {\n        'name': name,\n        'price': price,\n        'description': description,\n        'links': links,\n        'in_stock': in_stock\n    }\n```",
      "expectations": [
        "Flags parsing HTML with regex as the primary anti-pattern: regex cannot reliably parse HTML because HTML is not a regular language; attribute order can vary, whitespace can differ, and nested tags break simple patterns (Ch 2: use BeautifulSoup or lxml, not regex, for HTML parsing)",
        "Flags that the price regex `\\$([\\d\\.]+)` will fail silently on prices with commas (e.g., $1,299.99) or different currency formats without any warning (Ch 2: parse defensively)",
        "Flags the description regex with `re.DOTALL` will incorrectly merge content from separate `<p>` tags that contain nested HTML tags like `<strong>` or `<a>` (Ch 2: regex cannot handle nested HTML)",
        "Flags the link extraction regex `href=[\"\\']([^\"\\']+)[\"\\']` will match hrefs in script tags, style tags, and HTML comments, returning many false positives (Ch 2: use a parser with proper DOM traversal)",
        "Flags no error handling on `requests.get()` and no status code check (Ch 1, 14: check response.raise_for_status())",
        "Flags no session usage for connection pooling (Ch 10: use requests.Session())",
        "Recommends replacing all regex parsing with BeautifulSoup CSS selectors or XPath, providing a corrected example using soup.select_one() and soup.select()"
      ]
    },
    {
      "id": "eval-03-clean-scraper-session-retry-css-selectors",
      "prompt": "Review this web scraper:\n\n```python\nimport logging\nimport time\nfrom urllib.robotparser import RobotFileParser\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nfrom bs4 import BeautifulSoup\n\nlogger = logging.getLogger(__name__)\n\nUSER_AGENT = 'ResearchBot/1.0 (contact: bot@example.com)'\nREQUEST_DELAY = 1.5  # seconds between requests\n\n\ndef build_session() -> requests.Session:\n    session = requests.Session()\n    session.headers['User-Agent'] = USER_AGENT\n    retry = Retry(\n        total=3,\n        backoff_factor=1,\n        status_forcelist=[429, 500, 502, 503, 504]\n    )\n    session.mount('https://', HTTPAdapter(max_retries=retry))\n    return session\n\n\ndef can_fetch(base_url: str, path: str) -> bool:\n    rp = RobotFileParser()\n    rp.set_url(f'{base_url}/robots.txt')\n    rp.read()\n    return rp.can_fetch(USER_AGENT, f'{base_url}{path}')\n\n\ndef parse_listing(html: str) -> list[dict]:\n    soup = BeautifulSoup(html, 'html.parser')\n    items = []\n    for card in soup.select('article.product-card'):\n        title_el = card.select_one('h2.product-title')\n        price_el = card.select_one('span.price')\n        if title_el is None or price_el is None:\n            logger.warning('Skipping card with missing elements')\n            continue\n        items.append({\n            'title': title_el.get_text(strip=True),\n            'price': price_el.get_text(strip=True),\n        })\n    return items\n\n\ndef scrape_category(base_url: str, category_path: str) -> list[dict]:\n    if not can_fetch(base_url, category_path):\n        logger.error('robots.txt disallows scraping %s', category_path)\n        return []\n\n    session = build_session()\n    all_items: list[dict] = []\n    page = 1\n\n    while True:\n        url = f'{base_url}{category_path}?page={page}'\n        try:\n            resp = session.get(url, timeout=10)\n            resp.raise_for_status()\n        except requests.RequestException as exc:\n            logger.error('Request failed for %s: %s', url, exc)\n            break\n\n        items = parse_listing(resp.text)\n        if not items:\n            break\n\n        logger.info('Page %d: extracted %d items', page, len(items))\n        all_items.extend(items)\n        page += 1\n        time.sleep(REQUEST_DELAY)\n\n    return all_items\n```",
      "expectations": [
        "Recognizes this is a well-structured, responsible scraper and says so explicitly",
        "Praises robots.txt check via `RobotFileParser` before any requests are made (Ch 18: always check and honor robots.txt)",
        "Praises the descriptive User-Agent with contact information making the bot identifiable (Ch 14: identify yourself with a descriptive User-Agent)",
        "Praises `requests.Session()` with a `Retry` adapter providing automatic retry on transient server errors and rate-limit responses (Ch 14, 10: sessions with retry logic)",
        "Praises CSS selectors via `soup.select()` and `soup.select_one()` instead of regex for HTML parsing (Ch 2: use BeautifulSoup CSS selectors)",
        "Praises defensive None checks on extracted elements before accessing text, with a warning log for skipped cards (Ch 2: parse defensively)",
        "Praises `resp.raise_for_status()` and catching `requests.RequestException` for all HTTP/network errors (Ch 1, 14: handle connection errors, timeouts, and HTTP errors)",
        "Praises `time.sleep(REQUEST_DELAY)` between pages to be polite to the server (Ch 14: rate limit requests)",
        "Praises structured logging of page number and item counts at each step (Ch 5: log progress)",
        "Does NOT manufacture issues to appear thorough; any suggestions are explicitly framed as minor optional improvements"
      ]
    }
  ]
}
